Rehana Akter- 2018-1-60-202
Umma Sahorina Sultana-2018-1-60-145

# Dataset :

# pima_diabetes.csv
#using feed forward neural network finding the accuracy

import torch
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedShuffleSplit
from torch.utils.data import Dataset, DataLoader
diabetes = pd.read_csv('pima_diabetes.csv')#load dataset
diabetes.head()
x = diabetes.iloc[:, :-1].values# features
y = diabetes.iloc[:, -1].values#targets
# separate the x and y data into training, test and validation :
ss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.4, random_state = 31)
for train_index, group_index in ss.split(x, y):
    xtrain, xgroup = x[train_index], x[group_index]
    ytrain, ygroup = y[train_index], y[group_index]

ss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = 31)
for test_index, valid_index in ss.split(xgroup, ygroup):
    xtest, xvalid = xgroup[test_index], xgroup[valid_index]
    ytest, yvalid = ygroup[test_index], ygroup[valid_index]
# let us standardize the features or the x values:
sc = StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.transform(xtest)
xvalid = sc.transform(xvalid)
# convert them into tensors:
xtrain, xtest, xvalid = map(torch.tensor, (xtrain, xtest, xvalid))
ytrain, ytest, yvalid = map(torch.tensor, (ytrain, ytest, yvalid))
# make the tensors dtype as float for x values only:
xtrain = xtrain.float()
xtest = xtest.float()
xvalid = xvalid.float()
# prepare to load the xtrain dataset by dividing it into small part:
class dds(Dataset):
    def __init__(self, x, y):
     self.x = x
     self.y = y
    def __getitem__(self, index):
      return self.x[index], self.y[index]
    def __len__(self):
      return len(self.x)
mydata = dds(xtrain, ytrain)
train_loader = DataLoader(dataset = mydata, batch_size=32)
# create a neural network architecture:
class FeedForward(torch.nn.Module):
    def __init__(self, n_inputs, n_outputs, hidden_size):#the higher the hidden size the better the result
        super(FeedForward, self).__init__()
        self.inputs = n_inputs
        self.outputs = n_outputs
        self.hidden = hidden_size#number of neurons
    # define the layers:
        self.fc1 = torch.nn.Linear(self.inputs, self.hidden)
        self.fc2 = torch.nn.Linear(self.hidden, self.hidden)
        self.fc3 = torch.nn.Linear(self.hidden, self.hidden)
        self.fc4 = torch.nn.Linear(self.hidden, self.hidden)
        self.fc5 = torch.nn.Linear(self.hidden, self.outputs)

    # define the activation function:#as activation function we have used rectified linear unit
        self.relu = torch.nn.ReLU()#if the input is 0 output will be 0 for other case ouput will be same as input
    # initialize the weights:
    #def init_weights(self):
        self.init_weights()


    def init_weights(self):
     torch.nn.init.kaiming_normal_(self.fc1.weight)
     torch.nn.init.kaiming_normal_(self.fc2.weight)
     torch.nn.init.kaiming_normal_(self.fc3.weight)
     torch.nn.init.kaiming_normal_(self.fc4.weight)
     torch.nn.init.kaiming_normal_(self.fc5.weight)


    def forward(self, features):
     output = self.fc1(features)#1st hidden layer
     output = self.relu(output)#put in activation function
     output = self.fc2(output)
     output = self.relu(output)
     output = self.fc3(output)
     output = self.relu(output)
     output = self.fc4(output)
     output = self.relu(output)
     output = self.fc5(output)
     return output


# create a network object:
model = FeedForward(n_inputs = xtrain.shape[1], n_outputs = 2, hidden_size = 100)
# create a loss criteria:
criterion = torch.nn.CrossEntropyLoss(reduction = 'mean')
# create an optimization or gradient:
optimizer = torch.optim.SGD(params = model.parameters(), lr = .0001, momentum = 0.9)

# Train the network:

iteration = 300
valid_list = [0]


for epoch in range(iteration):
    for features, targets in train_loader:
        # forward propagation:
        output = model.forward(features)
        # calculate the loss:
        loss = criterion(output, targets)
        # initialize the gradient to zero:
        optimizer.zero_grad()
        # back propagation:
        loss.backward()
        # update the weight:
        optimizer.step()

   # model.eval()  # stop the training:

    c = torch.argmax(output.data, dim = 1)
    train_accuracy = (c == targets).sum().item() / targets.shape[0]
   # print('accuracy', train_accuracy)

    # use the model to predict validation set:
    predict = model.forward(xvalid)
    c1 = torch.argmax(predict.data, dim = 1)
    valid_accuracy = (c1 == yvalid).sum().item() / yvalid.shape[0]

    print("Iteration {}/{}, Loss: {: .4f}, train_accuracy: {: .4f} \, valid_accuracy: {: .4f}".format(epoch, iteration, loss, train_accuracy,
                                                                                           valid_accuracy))

    #model.train()

    # Early Stopping Machanism:
    valid_list.append(valid_accuracy)

# if valid_list[-1] < valid_list[-2]:#if negative, increase
#   patience += 1
#  if patience > 3:
#     break

# if valid_list[-1] > valid_list[-2]:#if positive, decrease
#   patience -= 1
#  if patience < 0:
#     patience = 0

with torch.no_grad():
    predict = model.forward(xtest)
    c = torch.argmax(predict.data, dim = 1)
    test_accuracy = (c == ytest).sum().item() / ytest.shape[0]

    print('Testing Accuracy is: {:.4f}'.format(test_accuracy))